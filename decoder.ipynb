{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e74633fa-bd6d-49b7-ba58-c3e6e142c3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "ce18f2ab-69b6-4d4e-886e-d2a53d9c7121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positional_encoder(d, N, R = 1000):\n",
    "    \"\"\"\n",
    "    See:\n",
    "        https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "        https://en.wikipedia.org/wiki/Transformer_(machine_learning_model). \n",
    "    R should be much larger than d. Wikipedia calls R -> N.\n",
    "    \"\"\"\n",
    "    r = np.power(R,2/d)\n",
    "    ts = torch.range(0,N-1).repeat(int(d/2),1)\n",
    "    ks = torch.transpose(torch.range(0,d/2-1).repeat(N,1), 0,1)\n",
    "    thetas = ts/np.power(r,ks)\n",
    "    return torch.stack(\n",
    "        (torch.sin(thetas),torch.cos(thetas)), dim=1).view(d,N).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "be0b6ef4-1388-488c-910d-127f55ecc1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask(b, N):\n",
    "        \"\"\"\n",
    "        bxNxN mask that is true for i <= j\n",
    "        \"\"\"\n",
    "        return (torch.range(1,N).repeat(N,1) <= torch.range(1,N).repeat(N,1).transpose(0,1)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "3b30c32d-c675-497f-ba06-0bc9730b48b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (b = batch size, N = sequence length, M = embedding size)\n",
    "\n",
    "class AttnHead(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super().__init__()\n",
    "        self.WQ = nn.Linear(in_features = M, out_features = M, bias = False)\n",
    "        self.WK = nn.Linear(in_features = M, out_features = M, bias = False)\n",
    "        self.WV = nn.Linear(in_features = M, out_features = M, bias = False)\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        \n",
    "    def create_mask(self):\n",
    "        \"\"\"\n",
    "        bxNxN mask that is true for i <= j\n",
    "        \"\"\"\n",
    "        return (torch.range(1,self.N).repeat(self.N,1) <= torch.range(1,self.N).repeat(self.N,1).transpose(0,1)\n",
    "            ).repeat(1,1,1)\n",
    " \n",
    "    def dot_product_attn(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        Q, K, V should be NxM\n",
    "        \"\"\"\n",
    "        d = K.shape[1]\n",
    "        N = Q.shape[0]\n",
    "        return torch.matmul(\n",
    "            torch.nn.Softmax(1)(\n",
    "                torch.matmul(Q, torch.transpose(K,1,2)) / np.sqrt(d)\n",
    "            # ) * self.create_mask()\n",
    "            ), \n",
    "            V) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + positional_encoder(self.M, self.N)\n",
    "        Q, K, V = self.WQ(x), self.WK(x), self.WV(x)\n",
    "        return self.dot_product_attn(Q,K,V).mean(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a03a0874-2de9-43bd-9264-123bca803458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# B = number of samples\n",
    "\n",
    "class random_reverse(Dataset):\n",
    "    def __init__(self, B, N, M):\n",
    "        rand_data = torch.normal(mean = torch.zeros(B,N,1))\n",
    "        self.data = rand_data.repeat(1,1,M)\n",
    "        self.labels = self.data.mean(2).flip(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b9543-70a8-4d7a-b60a-545e59b8dea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rr = random_reverse(100000,3,10)\n",
    "batch_size = 512\n",
    "data_loader = DataLoader(rr, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "head = AttnHead(3,10)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(head.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = head(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
